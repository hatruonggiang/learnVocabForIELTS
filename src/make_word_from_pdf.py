import os
from pdf2image import convert_from_path
import pytesseract
import nltk
from nltk.corpus import stopwords
from collections import Counter
import pandas as pd
import string
import re
from nltk.corpus import words as nltk_words
from nltk.corpus import wordnet as wn

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/words')
except LookupError:
    nltk.download('words')

print("T·∫£i xu·ªëng t√†i nguy√™n ho√†n t·∫•t.")

# C·∫•u h√¨nh Tesseract n·∫øu c·∫ßn (n·∫øu ƒë√£ th√™m v√†o PATH th√¨ kh√¥ng c·∫ßn)
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# ƒë√£ xong - c·∫•m s·ª≠a: chuy·ªÉn 1 pdf sang d·∫°ng ·∫£nh, r·ªìi d√πng OCR ƒë·ªÉ ƒë·ªçc pdf
def extract_text_from_image_pdf(pdf_path, max_pages=10):
    print(f"üìÑ Converting up to {max_pages} pages to images...")
    images = convert_from_path(pdf_path, first_page=1, last_page=max_pages)
    text = ""
    for i, img in enumerate(images):
        print(f"üîç OCR page {i+1}/{len(images)}...")
        text += pytesseract.image_to_string(img)
    return text.strip()

# ƒë√£ xong - c·∫•m s·ª≠a: l·ªçc t·ª´ scan nh·∫ßm b·∫±ng c√°ch check xem xong t·ª´ ƒëi·ªÉn ti·∫øng anh c√≥ n√≥ k, 
def clean_and_tokenize(text, vi_dict_path):
    if not isinstance(vi_dict_path, str):
        raise ValueError(f"Expected string path, got {type(vi_dict_path)}")

    if not os.path.exists(vi_dict_path):
        raise FileNotFoundError(f"File not found: {vi_dict_path}")

    # Load t·ª´ ƒëi·ªÉn t·ª´ file
    vi_dict = load_vi_dict_from_txt(vi_dict_path)
    english_words_in_dict = set(vi_dict.keys())

    stop_words = set(stopwords.words('english'))
    text = text.lower()

    words = re.findall(r'\b[a-zA-Z]+\b', text)

    filtered_words = []
    for word in words:
        if len(word) > 2 and word not in stop_words:
            if word in english_words_in_dict and len(word) == len(word.strip()):
                filtered_words.append(word)


    print(f"‚úÖ T·ª´ gi·ªØ l·∫°i: {filtered_words}")


    return filtered_words






# ƒë√£ xong - c·∫•m s·ª≠a: x·∫øp lo·∫°i c√°c t·ª´ theo t·∫ßn xu·∫•t xu·∫•t hi·ªán trong file
def analyze_vocab(words):
    counter = Counter(words)
    vocab_list = list(counter.items())
    df = pd.DataFrame(vocab_list, columns=['word', 'frequency'])
    df = df.sort_values(by="frequency", ascending=False).reset_index(drop=True)
    return df



# d·ªãch ti·∫øng anh sang ti·∫øng vi·ªát
def load_vi_dict_from_txt(file_path: str) -> dict:
    vi_dict = {}
    current_word = ""
    current_def = []

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("|"):  # D√≤ng m·ªõi b·∫Øt ƒë·∫ßu v·ªõi t·ª´ m·ªõi
                if current_word and current_def:
                    # Gh√©p nghƒ©a theo ƒë·ªãnh d·∫°ng "+) nghƒ©a" tr∆∞·ªõc khi l∆∞u
                    meaning_str = "\n+) " + "\n+) ".join(current_def[:3])  # Gi·ªõi h·∫°n 3 nghƒ©a
                    vi_dict[current_word.lower()] = meaning_str.strip()
                current_word = line[1:].strip()
                current_def = []
            elif line.startswith("-") or line.startswith("+"):
                current_def.append(line[1:].strip())
        
        # ƒê·ª´ng qu√™n l∆∞u t·ª´ cu·ªëi c√πng
        if current_word and current_def:
            meaning_str = "\n+) " + "\n+) ".join(current_def[:3])  # Gi·ªõi h·∫°n 3 nghƒ©a
            vi_dict[current_word.lower()] = meaning_str.strip()
    
    return vi_dict



# th√™m nghƒ©a ti·∫øng anh v√† 1 v√†i v√≠ d·ª• t·ªá h·∫°i, v√≠ d·ª• x·ªãn th√¨ duy·ªát l√¢u l·∫Øm, l√¢u g·∫•p c·ª° 5000 l·∫ßn
def get_word_details(words: list, vi_dict: dict = None) -> list:
    results = []

    for word in words:
        synsets = wn.synsets(word)

        if not synsets:
            results.append({
                'word': word,
                'meaning': '',
                'part_of_speech': '',
                'examples': '',
                'phonetic': '',
                'count': 0,
                'meaning_vi': vi_dict.get(word.lower(), '') if vi_dict else '',
            })
            continue

        meanings = []
        pos_list = set()
        example_sentences = []
        phonetic = synsets[0].lemmas()[0].name()

        for syn in synsets:
            meanings.append(syn.definition())
            pos_list.add(syn.pos())
            example_sentences.extend(syn.examples())  # ‚úÖ d√πng extend thay v√¨ append

        meaning_str = "\n+) " + "\n+) ".join(list(meanings)[:1])
        examples_str = "\n+) " + "\n+) ".join(list(example_sentences)[:3])
        pos_str = ", ".join(pos_list)

        results.append({
            'word': word,
            'meaning': meaning_str,
            'part_of_speech': pos_str,
            'examples': examples_str,
            'phonetic': phonetic,
            'count': 0,
            'meaning_vi': vi_dict.get(word.lower(), '') if vi_dict else ''
        })

    return results



# Ki·ªÉm tra n·∫øu file master ƒë√£ t·ªìn t·∫°i
def initialize_master_file(master_file):
    try:
        # N·∫øu file ƒë√£ t·ªìn t·∫°i th√¨ ƒë·ªçc v√†o
        master_df = pd.read_csv(master_file)
    except FileNotFoundError:
        # N·∫øu ch∆∞a t·ªìn t·∫°i, t·∫°o file tr·ªëng v·ªõi c√°c c·ªôt: word, meaning, part_of_speech, examples
        master_df = pd.DataFrame(columns=['word', 'meaning', 'part_of_speech', 'examples', 'meaning_vi'])
        master_df.to_csv(master_file, index=False)  # T·∫°o file m·ªõi
    return master_df





def save_to_csv(df, output_path):
    df.to_csv(output_path, index=False)
    print(f"‚úÖ Vocabulary saved to: {output_path}")

def process_pdf(pdf_path,master_file, output_csv,vi_dict_path):
    vi_dict = load_vi_dict_from_txt(vi_dict_path)
    master_df = initialize_master_file(master_file)
    text = extract_text_from_image_pdf(pdf_path, 30)
    words = clean_and_tokenize(text, vi_dict_path)
    vocab_df = analyze_vocab(words)
    # vocab_df = vocab_df[~vocab_df['word'].isin(master_df['word'])]
    # √Åp d·ª•ng h√†m get_word_details cho c·ªôt 'word' c·ªßa vocab_df
    word_details = get_word_details(vocab_df['word'].tolist(), vi_dict=vi_dict)

    # Chuy·ªÉn th√¥ng tin chi ti·∫øt th√†nh DataFrame
    details_df = pd.DataFrame(word_details)
    master_df = pd.concat([master_df, details_df], ignore_index=True)
    master_df.to_csv(master_file, index=False)

    # G·ªôp th√¥ng tin chi ti·∫øt v√†o vocab_df
    if not details_df.empty and 'word' in details_df.columns:
        vocab_df = vocab_df.join(details_df.set_index('word'), on='word')
        vocab_df = vocab_df.drop(columns=['frequency'])
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ chi ti·∫øt t·ª´ v·ª±ng ƒë·ªÉ enrich.")
    save_to_csv(vocab_df, output_csv)
    return vocab_df
    