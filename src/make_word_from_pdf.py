import os
from pdf2image import convert_from_path
import pytesseract
import nltk
from nltk.corpus import stopwords
from collections import Counter
import pandas as pd
import string
import re
from nltk.corpus import words as nltk_words
from nltk.corpus import wordnet as wn


# ƒê·∫£m b·∫£o NLTK data ƒë√£ ƒë∆∞·ª£c t·∫£i
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')

# C·∫•u h√¨nh Tesseract n·∫øu c·∫ßn (n·∫øu ƒë√£ th√™m v√†o PATH th√¨ kh√¥ng c·∫ßn)
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# ƒë√£ xong - c·∫•m s·ª≠a: chuy·ªÉn 1 pdf sang d·∫°ng ·∫£nh, r·ªìi d√πng OCR ƒë·ªÉ ƒë·ªçc pdf
def extract_text_from_image_pdf(pdf_path, max_pages=10):
    print(f"üìÑ Converting up to {max_pages} pages to images...")
    images = convert_from_path(pdf_path, first_page=1, last_page=max_pages)
    text = ""
    for i, img in enumerate(images):
        print(f"üîç OCR page {i+1}/{len(images)}...")
        text += pytesseract.image_to_string(img)
    return text.strip()

# ƒë√£ xong - c·∫•m s·ª≠a: l·ªçc t·ª´ scan nh·∫ßm b·∫±ng c√°ch check xem xong t·ª´ ƒëi·ªÉn ti·∫øng anh c√≥ n√≥ k, 
def clean_and_tokenize(text, vi_dict_path):
    # Ki·ªÉm tra vi_dict_path c√≥ ph·∫£i l√† ƒë∆∞·ªùng d·∫´n t·ªáp hay kh√¥ng
    if not isinstance(vi_dict_path, str):
        raise ValueError(f"Expected string path, got {type(vi_dict_path)}")
    
    # Ki·ªÉm tra n·∫øu file kh√¥ng t·ªìn t·∫°i
    if not os.path.exists(vi_dict_path):
        raise FileNotFoundError(f"File not found: {vi_dict_path}")

    vi_dict = load_vi_dict_from_txt(vi_dict_path)  # N·∫°p t·ª´ ƒëi·ªÉn t·ª´ t·ªáp
    english_keys = set(vi_dict.keys())  # L·∫•y danh s√°ch c√°c t·ª´ ti·∫øng Anh t·ª´ vi_dict

    stop_words = set(stopwords.words('english'))
    text = text.lower()  # Chuy·ªÉn vƒÉn b·∫£n th√†nh ch·ªØ th∆∞·ªùng

    # S·ª≠ d·ª•ng regex ƒë·ªÉ l·ªçc t·ª´ h·ª£p l·ªá (ch·ªâ t·ª´ ch·ª©a ch·ªØ c√°i a-z)
    words = re.findall(r'\b[a-zA-Z]+\b', text)

    filtered_words = []
    word_dict = {}  # T·∫°o bi·∫øn l∆∞u tr·ªØ t·ª´ ƒë√£ t√¨m th·∫•y trong t·ª´ ƒëi·ªÉn
    word_lengths = {}  # L∆∞u tr·ªØ ƒë·ªô d√†i c·ªßa t·ª´ trong word_dict

    word_dict = {}

    for word in words:
        if len(word) > 1 and word not in stop_words:
            for dict_word in vi_dict:
                if word == dict_word:  # Ch·ªâ l·∫•y n·∫øu t·ª´ kh·ªõp ho√†n to√†n
                    word_dict[word] = dict_word  # L∆∞u l·∫°i key t·ª´ ƒëi·ªÉn
                    filtered_words.append(word)
                    break

    
    return filtered_words






# ƒë√£ xong - c·∫•m s·ª≠a: x·∫øp lo·∫°i c√°c t·ª´ theo t·∫ßn xu·∫•t xu·∫•t hi·ªán trong file
def analyze_vocab(words):
    counter = Counter(words)
    vocab_list = list(counter.items())
    df = pd.DataFrame(vocab_list, columns=['word', 'frequency'])
    df = df.sort_values(by="frequency", ascending=False).reset_index(drop=True)
    return df



# d·ªãch ti·∫øng anh sang ti·∫øng vi·ªát
def load_vi_dict_from_txt(file_path: str) -> dict:
    vi_dict = {}
    current_word = ""
    current_def = []

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("|"):  # D√≤ng m·ªõi b·∫Øt ƒë·∫ßu v·ªõi t·ª´ m·ªõi
                if current_word and current_def:
                    # Gh√©p nghƒ©a theo ƒë·ªãnh d·∫°ng "+) nghƒ©a" tr∆∞·ªõc khi l∆∞u
                    meaning_str = "\n+) " + "\n+) ".join(current_def[:5])  # Gi·ªõi h·∫°n 3 nghƒ©a
                    vi_dict[current_word.lower()] = meaning_str.strip()
                current_word = line[1:].strip()
                current_def = []
            elif line.startswith("-") or line.startswith("+"):
                current_def.append(line[1:].strip())
        
        # ƒê·ª´ng qu√™n l∆∞u t·ª´ cu·ªëi c√πng
        if current_word and current_def:
            meaning_str = "\n+) " + "\n+) ".join(current_def[:5])  # Gi·ªõi h·∫°n 3 nghƒ©a
            vi_dict[current_word.lower()] = meaning_str.strip()
    
    return vi_dict



# th√™m nghƒ©a ti·∫øng anh v√† 1 v√†i v√≠ d·ª• t·ªá h·∫°i, v√≠ d·ª• x·ªãn th√¨ duy·ªát l√¢u l·∫Øm, l√¢u g·∫•p c·ª° 5000 l·∫ßn
def get_word_details(words: list, vi_dict: dict = None) -> list:
    results = []

    for word in words:
        synsets = wn.synsets(word)

        if not synsets:
            results.append({
                'word': word,
                'meaning': '',
                'part_of_speech': '',
                'examples': '',
                'phonetic': '',
                'meaning_vi': vi_dict.get(word.lower(), '') if vi_dict else ''
            })
            continue

        meanings = []
        pos_list = set()
        example_sentences = []
        phonetic = synsets[0].lemmas()[0].name()

        for syn in synsets:
            meanings.append(syn.definition())
            pos_list.add(syn.pos())
            example_sentences.extend(syn.examples())  # ‚úÖ d√πng extend thay v√¨ append

        meaning_str = "\n+) " + "\n+) ".join(list(meanings)[:3])
        examples_str = "\n+) " + "\n+) ".join(list(example_sentences)[:3])
        pos_str = ", ".join(pos_list)

        results.append({
            'word': word,
            'meaning': meaning_str,
            'part_of_speech': pos_str,
            'examples': examples_str,
            'phonetic': phonetic,
            'meaning_vi': vi_dict.get(word.lower(), '') if vi_dict else ''
        })

    return results



# Ki·ªÉm tra n·∫øu file master ƒë√£ t·ªìn t·∫°i
def initialize_master_file(master_file):
    try:
        # N·∫øu file ƒë√£ t·ªìn t·∫°i th√¨ ƒë·ªçc v√†o
        master_df = pd.read_csv(master_file)
    except FileNotFoundError:
        # N·∫øu ch∆∞a t·ªìn t·∫°i, t·∫°o file tr·ªëng v·ªõi c√°c c·ªôt: word, meaning, part_of_speech, examples
        master_df = pd.DataFrame(columns=['word', 'meaning', 'part_of_speech', 'examples', 'meaning_vi'])
        master_df.to_csv(master_file, index=False)  # T·∫°o file m·ªõi
    return master_df





def save_to_csv(df, output_path):
    df.to_csv(output_path, index=False)
    print(f"‚úÖ Vocabulary saved to: {output_path}")

def process_pdf(pdf_path,master_file, output_csv,vi_dict_path):
    vi_dict = load_vi_dict_from_txt(vi_dict_path)
    master_df = initialize_master_file(master_file)
    text = extract_text_from_image_pdf(pdf_path, 30)
    words = clean_and_tokenize(text, vi_dict_path)
    vocab_df = analyze_vocab(words)
    # vocab_df = vocab_df[~vocab_df['word'].isin(master_df['word'])]
    # √Åp d·ª•ng h√†m get_word_details cho c·ªôt 'word' c·ªßa vocab_df
    word_details = get_word_details(vocab_df['word'].tolist(), vi_dict=vi_dict)

    # Chuy·ªÉn th√¥ng tin chi ti·∫øt th√†nh DataFrame
    details_df = pd.DataFrame(word_details)
    master_df = pd.concat([master_df, details_df], ignore_index=True)
    master_df.to_csv(master_file, index=False)

    # G·ªôp th√¥ng tin chi ti·∫øt v√†o vocab_df
    if not details_df.empty and 'word' in details_df.columns:
        vocab_df = vocab_df.join(details_df.set_index('word'), on='word')
        vocab_df = vocab_df.drop(columns=['frequency'])
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ chi ti·∫øt t·ª´ v·ª±ng ƒë·ªÉ enrich.")
    save_to_csv(vocab_df, output_csv)
    return vocab_df
    


# üü© G·ªçi h√†m ch√≠nh: duy·ªát folder PDF
if __name__ == "__main__":
    input_folder = "data/pdfs"
    output_folder = "data/outputs"
    master_file = "data/master.csv"
    vi_dict_path = "data/pdfs/data.txt"


    for filename in os.listdir(input_folder):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(input_folder, filename)
            base_name = os.path.splitext(filename)[0]
            output_csv = os.path.join(output_folder, f"{base_name}.csv")

            if not os.path.exists(output_csv):
                print(f"\nüöÄ ƒêang x·ª≠ l√Ω: {filename}")
                process_pdf(pdf_path,master_file, output_csv,vi_dict_path)
            else:
                print(f"‚è≠Ô∏è B·ªè qua (ƒë√£ c√≥ CSV): {filename}")